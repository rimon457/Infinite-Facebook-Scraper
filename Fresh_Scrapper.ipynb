{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T11:32:16.192848Z",
     "start_time": "2019-03-01T11:31:52.312482Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time,threading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium.webdriver import ChromeOptions\n",
    "#from selenium.webdriver.support import expected_conditions as EC\n",
    "#adding custom headers so that FB can't detect this as a bot\n",
    "opts=ChromeOptions()\n",
    "opts.add_argument(\"--incognito\")\n",
    "opts.add_argument(\"--Host= https://www.google.com/\")\n",
    "opts.add_argument(\"--Connection= keep-alive\")\n",
    "opts.add_argument(\"--Accept= text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\")\n",
    "opts.add_argument(\"--Accept-Encoding= gzip, deflate, sdch\")\n",
    "opts.add_argument(\"--Accept-Language= en-US,en;q=0.8\")\n",
    "opts.add_argument(\"--Referrer= https://www.google.com/\")\n",
    "opts.add_argument(\"--disable-infobars\")\n",
    "#browser= webdriver.Firefox(executable_path='C:/Users/r.m.karim/Desktop/Webdrivers/geckodriver')\n",
    "\n",
    "#how_many_clicks=1500\n",
    "#ee_more_posts=browser.find_element_by_xpath('//*[@id=\"m_more_item\"]/a')\n",
    "# to try to scrape for unlimited time....................................................\n",
    "flag=1\n",
    "file=pd.read_csv(\"prox.txt\",sep='\\t')\n",
    "#random UA generator\n",
    "def get_random_ua():\n",
    "    random_ua = ''\n",
    "    ua_file = 'ua_file.txt'\n",
    "    try:\n",
    "        with open(ua_file) as f:\n",
    "            lines = f.readlines()\n",
    "        if len(lines) > 0:\n",
    "            prng = np.random.RandomState()\n",
    "            index = prng.permutation(len(lines) - 1)\n",
    "            idx = np.asarray(index, dtype=np.integer)[0]\n",
    "            random_ua = lines[int(idx)]\n",
    "    except Exception as ex:\n",
    "        print('Exception in random_ua')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return random_ua\n",
    "\n",
    "opts.add_argument(\"--user-agent={}\".format(get_random_ua()))\n",
    "#opts.add_argument(\"--headless\")\n",
    "#random_ip=np.random.choice(file[\"IP Address\"])\n",
    "#opts.add_argument(\"--proxy-SERVER={0}:{1}\".format(random_ip,file.Port.loc[file[file[\"IP Address\"]==random_ip].index[0]]))\n",
    "browser = webdriver.Chrome('C:/Users/r.m.karim/Desktop/Webdrivers/chromedriver',options=opts)\n",
    "browser.get(\"https://m.facebook.com/groups/dsdbangladesh/\")\n",
    "browser.set_page_load_timeout(300)\n",
    "def browsing():\n",
    "    global flag\n",
    "    while flag==1:\n",
    "        opts.add_argument(\"--user-agent={}\".format(get_random_ua()))\n",
    "#       random_ip=np.random.choice(file[\"IP Address\"])\n",
    "#       opts.add_argument(\"--proxY-SERVER={0}:{1}\".format(random_ip,file.Port.loc[file[file[\"IP Address\"]==random_ip].index[0]]))\n",
    "        #browser = webdriver.Chrome('C:/Users/r.m.karim/Desktop/Webdrivers/chromedriver',options=opts)\n",
    "        #print(browser.execute_script(\"return navigator.userAgent;\"))\n",
    "        browser.implicitly_wait(120)\n",
    "        browser.find_element_by_xpath('//*[@id=\"m_more_item\"]/a').click() #chrome\n",
    "        #browser.find_element_by_xpath('/html/body/div/div/div[2]/div/div[1]/div[4]/div[2]/a').click()  #firefox\n",
    "        time.sleep(abs(np.random.normal(5))+1)\n",
    "        if flag==False:\n",
    "            source_data=browser.page_source\n",
    "            soup=bs(source_data,'lxml')\n",
    "            paragraph = [\"\".join(x.findAll(text=True)) for x in soup.findAll(\"p\")]\n",
    "            df=pd.DataFrame({'Comments':paragraph})\n",
    "            df.to_excel(\"DSD_Posts0.xls\",index=False)\n",
    "            print(\"Yes!Posts are scrapped!\")\n",
    "            print(\"restarting!sleeping for 20 seconds...\")\n",
    "            time.sleep(20)\n",
    "            clicks=100\n",
    "            while clicks <=100:\n",
    "                browser.find_element_by_xpath('//*[@id=\"m_more_item\"]/a').click()\n",
    "                time.sleep(abs(np.random.normal(5))+1)\n",
    "                clicks=+1\n",
    "            source_data=browser.page_source\n",
    "            soup=bs(source_data,'lxml')\n",
    "            paragraph = [\"\".join(x.findAll(text=True)) for x in soup.findAll(\"p\")]\n",
    "            df=pd.DataFrame({'Comments':paragraph})\n",
    "            df.to_excel(\"DSD_Posts1.xls\",index=False)\n",
    "            print(\"Yes!Posts are scrapped again!!\")\n",
    "            \n",
    "            \n",
    "def user_input():\n",
    "    global flag\n",
    "    keystrk=input('Please press ENTER to stop \\n')\n",
    "    # thread doesn't continue until key is pressed\n",
    "    print('Crawling is stopped!')\n",
    "    flag=False\n",
    "    print(\"Now processing the scraped data so far! please wait...\")\n",
    "n=threading.Thread(target=browsing)\n",
    "i=threading.Thread(target=user_input)\n",
    "n.start()\n",
    "i.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
